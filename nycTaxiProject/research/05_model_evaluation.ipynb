{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d7a092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b6679b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dagshub\n",
    "# dagshub.init(repo_owner='mann-lean', repo_name='data-science-project', mlflow=True)\n",
    "\n",
    "# import mlflow\n",
    "# with mlflow.start_run():\n",
    "#   mlflow.log_param('parameter name', 'value')\n",
    "#   mlflow.log_metric('metric name', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eb75548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set environment variables for MLFLOW\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"]=\"https://dagshub.com/mann-lean/data-science-project.mlflow\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"]=\"mann-lean\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"]=\"d48e826348084596921be95179ff847d63506cba\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5864b3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\dsProject\\\\nycTaxiProject\\\\research'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3f5b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f96d012a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\dsProject\\\\nycTaxiProject'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12176cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90471b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    x_train_dir: Path\n",
    "    y_train_dir: Path\n",
    "    x_test_dir: Path\n",
    "    y_test_dir: Path\n",
    "    model_dir: Path\n",
    "    all_params:dict\n",
    "    mlflow_uri:str\n",
    "    model_evaluation:Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "685d140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nycTaxiProject.util.common import read_yaml,create_directories,save_evaluation\n",
    "from nycTaxiProject.constants import CONFIG_FILE_PATH,PARAMS_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eabaf4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManger:\n",
    "    def __init__(\n",
    "            self,\n",
    "     config_file_path=CONFIG_FILE_PATH,\n",
    "     params_file_path=PARAMS_FILE_PATH\n",
    "     ):\n",
    "        self.config=read_yaml(config_file_path)\n",
    "        self.params=read_yaml(params_file_path)\n",
    "\n",
    "    def get_model_evaluation(self)->ModelEvaluationConfig:\n",
    "        config=self.config.model_evaluation\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config=ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            x_train_dir=config.x_train_dir,\n",
    "            y_train_dir=config.y_train_dir,\n",
    "            x_test_dir=config.x_test_dir,\n",
    "            y_test_dir=config.y_test_dir,\n",
    "            model_dir=config.model_dir,\n",
    "            all_params= self.params,\n",
    "            mlflow_uri= \"https://dagshub.com/mann-lean/data-science-project.mlflow\",\n",
    "            model_evaluation=config.model_evaluation\n",
    "        )\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e0ed7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\dsProject\\nycTaxiProject\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger=logging.getLogger(__name__)\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import dagshub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5272ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Evaluation:\n",
    "    def __init__(self,config=ModelEvaluationConfig):\n",
    "        self.config=config\n",
    "        self.model=joblib.load(self.config.model_dir) #importing model(.pkl )\n",
    "    def evaluate_model(self, X, y_true, name:str): #why argument  need model,when we are loading the model from the directory? because we are loading the model from the directory, we don't need to pass the model as an argument to the evaluate_model method. We can remove the model argument from the method definition and directly load the model within the method using joblib.load(self.config.model_dir). Here's how you can modify the evaluate_model method:\n",
    "        try:\n",
    "            #1 Initializing Dagshub logger\n",
    "            dagshub.init(repo_owner='mann-lean', repo_name='data-science-project', mlflow=True)\n",
    "            #2 set the tracking URI for MLflow\n",
    "            mlflow.set_tracking_uri(self.config.mlflow_uri)\n",
    "            model=self.model\n",
    "           \n",
    "            with mlflow.start_run():\n",
    "                y_pred = model.predict(X)\n",
    "                r2 = r2_score(y_true, y_pred)\n",
    "                rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "                mse=mean_squared_error(y_true, y_pred)\n",
    "                mae = mean_absolute_error(y_true, y_pred)\n",
    "                \n",
    "                print(f\"-----------{name} Evaluation-------------\")\n",
    "                print(f\"R2 Score:   {r2:.4f}\")\n",
    "                print(f\"RMSE:       {rmse:.4f}\")\n",
    "                print(f\"MSE:        {mse:.4f}\")\n",
    "                print(f\"MAE:        {mae:.4f}\")\n",
    "\n",
    "                # saving evaluation metrics in a dictionary\n",
    "                evalutaion_dict={\"name\":name,\n",
    "                                 \"R2_Score\":r2,\n",
    "                                \"RMSE\":rmse,\n",
    "                                \"MSE\":mse,\n",
    "                                \"MAE\":mae}\n",
    "                # saving evaluation metrics in a JSON file (LOCALLY)\n",
    "                save_evaluation(evalutaion_dict,Path(self.config.model_evaluation))\n",
    "                \n",
    "            # 4. LOG PARAMS: Save hyperparameter configurations (e.g., alpha, penalty)\n",
    "                if self.config.all_params:\n",
    "                    mlflow.log_params(self.config.all_params)\n",
    "            # 5. LOG METRICS: Log evaluation metrics (e.g., R2, RMSE, MSE, MAE)\n",
    "                mlflow.log_metric(f\"{name} R2_Score\", r2)\n",
    "                mlflow.log_metric(f\"{name} RMSE\", rmse)\n",
    "                mlflow.log_metric(f\"{name} MSE\", mse)\n",
    "                mlflow.log_metric(f\"{name} MAE\", mae)\n",
    "            # 6. LOG MODEL: Optionally, log the trained model itself for future reference\n",
    "                mlflow.sklearn.log_model(model, \"model\", registered_model_name=\"SGDRegressor\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf55e357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-25 19:49:19,809 : INFO : 2382430873 : >>>>>>>STAGE: MODEL EVALUATION Started<<<<<<<<< ]\n",
      "[2026-02-25 19:49:19,831 : INFO : common : yaml file: config\\config.yaml LOADED successfully]\n",
      "[2026-02-25 19:49:19,840 : INFO : common : yaml file: params.yaml LOADED successfully]\n",
      "[2026-02-25 19:49:19,844 : INFO : common : created directory at: artifacts/modelEvaluatioin]\n",
      "[2026-02-25 19:49:25,384 : INFO : _client : HTTP Request: GET https://dagshub.com/api/v1/repos/mann-lean/data-science-project \"HTTP/1.1 200 OK\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"mann-lean/data-science-project\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"mann-lean/data-science-project\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-25 19:49:25,403 : INFO : helpers : Initialized MLflow to track repo \"mann-lean/data-science-project\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository mann-lean/data-science-project initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository mann-lean/data-science-project initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-25 19:49:25,414 : INFO : helpers : Repository mann-lean/data-science-project initialized!]\n",
      "-----------training Evaluation-------------\n",
      "R2 Score:   0.6906\n",
      "RMSE:       2.2370\n",
      "MSE:        5.0043\n",
      "MAE:        1.1405\n",
      "[2026-02-25 19:49:27,094 : INFO : common : Evalution Metrices saved at artifacts\\modelEvaluatioin\\evaluation.json]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/25 19:49:29 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2026/02/25 19:49:33 WARNING mlflow.sklearn: Saving scikit-learn models in the pickle or cloudpickle format requires exercising caution because these formats rely on Python's object serialization mechanism, which can execute arbitrary code during deserialization. The recommended safe alternative is the 'skops' format. For more information, see: https://scikit-learn.org/stable/model_persistence.html\n",
      "Registered model 'SGDRegressor' already exists. Creating a new version of this model...\n",
      "2026/02/25 19:50:26 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: SGDRegressor, version 3\n",
      "Created version '3' of model 'SGDRegressor'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run upset-rat-681 at: https://dagshub.com/mann-lean/data-science-project.mlflow/#/experiments/0/runs/75a5ac4237e64a7385c2e82c339a9c06\n",
      "üß™ View experiment at: https://dagshub.com/mann-lean/data-science-project.mlflow/#/experiments/0\n",
      "[2026-02-25 19:50:32,023 : INFO : _client : HTTP Request: GET https://dagshub.com/api/v1/repos/mann-lean/data-science-project \"HTTP/1.1 200 OK\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"mann-lean/data-science-project\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"mann-lean/data-science-project\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-25 19:50:32,040 : INFO : helpers : Initialized MLflow to track repo \"mann-lean/data-science-project\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository mann-lean/data-science-project initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository mann-lean/data-science-project initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-25 19:50:32,050 : INFO : helpers : Repository mann-lean/data-science-project initialized!]\n",
      "-----------testing Evaluation-------------\n",
      "R2 Score:   0.7187\n",
      "RMSE:       2.0790\n",
      "MSE:        4.3223\n",
      "MAE:        1.1287\n",
      "[2026-02-25 19:50:32,863 : INFO : common : Evalution Metrices saved at artifacts\\modelEvaluatioin\\evaluation.json]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/25 19:50:35 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2026/02/25 19:50:40 WARNING mlflow.sklearn: Saving scikit-learn models in the pickle or cloudpickle format requires exercising caution because these formats rely on Python's object serialization mechanism, which can execute arbitrary code during deserialization. The recommended safe alternative is the 'skops' format. For more information, see: https://scikit-learn.org/stable/model_persistence.html\n",
      "Registered model 'SGDRegressor' already exists. Creating a new version of this model...\n",
      "2026/02/25 19:51:01 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: SGDRegressor, version 4\n",
      "Created version '4' of model 'SGDRegressor'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run brawny-whale-513 at: https://dagshub.com/mann-lean/data-science-project.mlflow/#/experiments/0/runs/c3ea21a98e1a421b9fe3b50c4dd08ac8\n",
      "üß™ View experiment at: https://dagshub.com/mann-lean/data-science-project.mlflow/#/experiments/0\n",
      "[2026-02-25 19:51:03,865 : INFO : 2382430873 : >>>>>>>STAGE: MODEL EVALUATION ENDED<<<<<<<<< ]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logger.info(\">>>>>>>STAGE: MODEL EVALUATION Started<<<<<<<<< \")\n",
    "    config=ConfigurationManger()\n",
    "    model_evaluation_config=config.get_model_evaluation()\n",
    "    x_train=pd.read_csv(model_evaluation_config.x_train_dir)\n",
    "    y_train=pd.read_csv(model_evaluation_config.y_train_dir)\n",
    "    model_evaluation=Model_Evaluation(model_evaluation_config)\n",
    "    model_evaluation.evaluate_model(x_train,y_train,\"training\")\n",
    "\n",
    "    x_test=pd.read_csv(model_evaluation_config.x_test_dir)\n",
    "    y_test=pd.read_csv(model_evaluation_config.y_test_dir)\n",
    "    model_evaluation.evaluate_model(x_test,y_test,\"testing\")\n",
    "    logger.info(\">>>>>>>STAGE: MODEL EVALUATION ENDED<<<<<<<<< \")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af412a49",
   "metadata": {},
   "source": [
    "##### -----------training Evaluation-------------<br>\n",
    "R2 Score:   0.6906<br>\n",
    "RMSE:       2.2370<br>\n",
    "MSE:        5.0043<br>\n",
    "MAE:        1.1405\n",
    "<br>\n",
    "\n",
    "##### -----------testing Evaluation-------------<br>\n",
    "R2 Score:   0.7187<br>\n",
    "RMSE:       2.0790<br>\n",
    "MSE:        4.3223<br>\n",
    "MAE:        1.1287"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4969876c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
